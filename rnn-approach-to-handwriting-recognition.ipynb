{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install textdistance\n!pip install Augmentor","metadata":{"execution":{"iopub.status.busy":"2022-03-07T07:24:30.686815Z","iopub.execute_input":"2022-03-07T07:24:30.687099Z","iopub.status.idle":"2022-03-07T07:24:46.954514Z","shell.execute_reply.started":"2022-03-07T07:24:30.687064Z","shell.execute_reply":"2022-03-07T07:24:46.953638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nimport math\nimport random\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import Conv2d, MaxPool2d, BatchNorm2d, ReLU, LeakyReLU\nfrom torch.utils.data import Dataset, sampler\nfrom torch.nn.utils.clip_grad import clip_grad_norm_\nfrom torchvision import models\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom textdistance import levenshtein as lev\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2022-03-07T07:24:46.956864Z","iopub.execute_input":"2022-03-07T07:24:46.957329Z","iopub.status.idle":"2022-03-07T07:24:51.605965Z","shell.execute_reply.started":"2022-03-07T07:24:46.957289Z","shell.execute_reply":"2022-03-07T07:24:51.605212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CONFIG","metadata":{}},{"cell_type":"code","source":"!mkdir ./checkpoints/\n\nALPHABET = \" %(),-./0123456789:;?[]«»АБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё-\"\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nREPORT_ACCURACY = True\nPATH_TO_TRAIN_IMGDIR = \"../input/cyrillic-handwriting-dataset/train/\"\nPATH_TO_TRAIN_LABELS = \"../input/cyrillic-handwriting-dataset/train.tsv\"\nPATH_TO_TEST_IMGDIR = \"../input/cyrillic-handwriting-dataset/test/\"\nPATH_TO_TEST_LABELS = \"../input/cyrillic-handwriting-dataset/test.tsv\"\nPATH_TO_CHECKPOINT = \"./checkpoints/\"\nBATCH_SIZE = 2\nAPPLY_AUGS = True # is augmentation applied?\nSEED = 41\n\ntorch.manual_seed(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T07:24:51.607286Z","iopub.execute_input":"2022-03-07T07:24:51.607552Z","iopub.status.idle":"2022-03-07T07:24:52.318512Z","shell.execute_reply.started":"2022-03-07T07:24:51.607507Z","shell.execute_reply":"2022-03-07T07:24:52.317627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MODELS\n\n### MODEL 1\n\n\"An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition\" https://arxiv.org/abs/1507.05717","metadata":{}},{"cell_type":"code","source":"class BidirectionalLSTM(nn.Module):\n\n    def __init__(self, nIn, nHidden, nOut):\n        super(BidirectionalLSTM, self).__init__()\n        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n        self.embedding = nn.Linear(nHidden * 2, nOut)\n    def forward(self, input):\n        self.rnn.flatten_parameters()\n        recurrent, _ = self.rnn(input)\n        T, b, h = recurrent.size()\n        t_rec = recurrent.view(T * b, h)\n        output = self.embedding(t_rec)  # [T * b, nOut]\n        output = output.view(T, b, -1)\n        return output\n\nclass Model1(nn.Module):\n\n    def __init__(self, nHidden, num_classes):\n        super(Model1, self).__init__()\n\n        self.conv0 = Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n        self.conv1 = Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.conv2 = Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.conv3 = Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n        self.conv4 = Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.conv5 = Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.conv6 = Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n        self.conv7 = Conv2d(512, 512, kernel_size=7, stride=1, padding=1)\n\n        self.pool1 = MaxPool2d(kernel_size=2, stride=2)\n        self.pool2 = MaxPool2d(kernel_size=2, stride=(2,1))\n        self.pool3 = MaxPool2d(kernel_size=2, stride=(2,1))\n        self.pool4 = MaxPool2d(kernel_size=2, stride=(3,1))\n\n        self.bn1 = nn.BatchNorm2d(64)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.bn4 = nn.BatchNorm2d(512)\n\n        self.rnn = nn.Sequential(\n            BidirectionalLSTM(nHidden*2, nHidden, nHidden),\n            BidirectionalLSTM(nHidden, nHidden, len(ALPHABET)))\n\n\n    def forward(self, src):\n        '''\n        src : [b, c, h, w]\n        '''\n        x = self.conv0(src)\n        x = self.bn1(self.pool1(self.conv1(x)))\n        x = self.conv2(x)\n        x = self.bn2(self.pool2(self.conv3(x)))\n        x = self.conv4(x)\n        x = self.bn3(self.pool3(self.conv5(x)))\n        x = self.conv6(x)\n        x = self.bn4(self.pool4(self.conv7(x)))\n        b, c, h, w = x.size() # [4, 512, 1, 121])\n        assert h == 1, \"the height of conv must be 1\"\n        x = x.squeeze(2) # [b, c, h*w]\n        x = x.permute(2, 0, 1)  # [h*w, b, c]\n        logits = self.rnn(x) # [h*w, b, num_classes]\n        output = torch.nn.functional.log_softmax(logits, 2)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-07T07:24:52.320327Z","iopub.execute_input":"2022-03-07T07:24:52.320702Z","iopub.status.idle":"2022-03-07T07:24:52.343694Z","shell.execute_reply.started":"2022-03-07T07:24:52.32066Z","shell.execute_reply":"2022-03-07T07:24:52.342936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MODEL 2: ResNet50 + LSTM","metadata":{}},{"cell_type":"code","source":"class Model2(nn.Module):\n\n    def __init__(self, nHidden, num_classes):\n        super(Model2, self).__init__()\n        self.resnet50 = models.resnet50(pretrained=True)\n        self.resnet50.fc1 = nn.Conv2d(2048, 512, kernel_size=(2, 2))\n        self.resnet50.fc2 = nn.Linear(8, 16)\n\n        self.rnn = nn.Sequential(\n            BidirectionalLSTM(nHidden*2, nHidden, nHidden),\n            BidirectionalLSTM(nHidden, nHidden, num_classes))\n\n\n    def forward(self, src):\n        # ResNet requires 3 channels\n        if src.shape[1] == 1:\n          src = src.repeat(1, 3, 1, 1)\n        x = self.resnet50.conv1(src)\n        x = self.resnet50.bn1(x)\n        x = self.resnet50.relu(x)\n        x = self.resnet50.maxpool(x)\n        x = self.resnet50.layer1(x)\n        x = self.resnet50.layer2(x)\n        x = self.resnet50.layer3(x)\n        x = self.resnet50.layer4(x)\n        x = self.resnet50.fc1(x)\n        b, c, h, w = x.size()\n        assert h == 1, \"the height of conv must be 1\"\n        x = x.squeeze(2) # [b, c, h*w]\n        x = x.permute(2, 0, 1)  # [h*w, b, c]\n        logits = self.rnn(x) # [h*w, b, num_classes]\n        output = torch.nn.functional.log_softmax(logits, 2)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-07T07:24:52.3484Z","iopub.execute_input":"2022-03-07T07:24:52.34868Z","iopub.status.idle":"2022-03-07T07:24:52.359699Z","shell.execute_reply.started":"2022-03-07T07:24:52.348655Z","shell.execute_reply":"2022-03-07T07:24:52.358828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MODEL 3 \n","metadata":{}},{"cell_type":"code","source":"class BidirectionalLSTM(nn.Module):\n\n    def __init__(self, nIn, nHidden, nOut):\n        super(BidirectionalLSTM, self).__init__()\n        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n        self.embedding = nn.Linear(nHidden * 2, nOut)\n    def forward(self, input):\n        self.rnn.flatten_parameters()\n        recurrent, _ = self.rnn(input)\n        T, b, h = recurrent.size()\n        t_rec = recurrent.view(T * b, h)\n        output = self.embedding(t_rec)  # [T * b, nOut]\n        output = output.view(T, b, -1)\n        return output\n\nclass Model3(nn.Module):\n\n    def __init__(self, nHidden, num_classes):\n        super(Model3, self).__init__()\n\n        self.conv0 = Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv1 = Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv2 = Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv3 = Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv4 = Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv5 = Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv6 = Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1))\n\n        self.pool0 = MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        self.pool1 = MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        self.pool3 = MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n        self.pool5 = MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n\n        self.bn2 = BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.bn4 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.bn6 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        self.relu = ReLU()\n\n        self.rnn = nn.Sequential(\n            BidirectionalLSTM(nHidden*2, nHidden, nHidden),\n            BidirectionalLSTM(nHidden, nHidden, num_classes))\n\n\n    def forward(self, src):\n        \n        x = self.pool0(self.relu(self.conv0(src)))\n        x = self.pool1(self.relu(self.conv1(x)))\n        x = self.relu(self.bn2(self.conv2(x)))\n        x = self.pool3(self.relu(self.conv3(x)))\n        x = self.relu(self.bn4(self.conv4(x)))\n        x = self.pool5(self.relu(self.conv5(x)))\n        x = self.relu(self.bn6(self.conv6(x)))\n\n        b, c, h, w = x.size()\n        assert h == 1, \"the height of conv must be 1\"\n        x = x.squeeze(2) # [b, c, h*w]\n        x = x.permute(2, 0, 1)  # [h*w, b, c]\n        logits = self.rnn(x) # [h*w, b, num_classes]\n        output = torch.nn.functional.log_softmax(logits, 2)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-07T07:24:52.362738Z","iopub.execute_input":"2022-03-07T07:24:52.363047Z","iopub.status.idle":"2022-03-07T07:24:52.385525Z","shell.execute_reply.started":"2022-03-07T07:24:52.363013Z","shell.execute_reply":"2022-03-07T07:24:52.384853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MODEL 4\n\n\"Fine-tuning Handwriting Recognition systems with Temporal Dropout\" https://arxiv.org/pdf/2102.00511v1.pdf","metadata":{}},{"cell_type":"code","source":"def TemporalDropout(x, p = 0.2):\n    B, C, WH = x.shape # BATCH_SIZE, CHANNELS, WIDTH*HEIGHT\n    v = torch.ones(size=(WH,))\n    for k in range(int(C*p)):\n      i = random.randint(0,WH-1)\n      v[i] = 0\n    mask = torch.stack([v]*C).to(DEVICE)\n    x = x*mask\n    return x\n\nclass Model4(nn.Module):\n\n    def __init__(self, nHidden, num_classes):\n        super(Model4, self).__init__()\n\n        self.act = LeakyReLU(negative_slope=0.01, inplace=False)\n        self.conv0 = Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n        self.conv1 = Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.conv2 = Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.conv3 = Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n        self.conv4 = Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.conv5 = Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.conv6 = Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.conv7 = Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n        self.conv8 = Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv9 = Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv10 = Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv11 = Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv12 = Conv2d(512, 512, kernel_size=4, stride=1, padding=1)\n\n        self.pool1 = MaxPool2d(kernel_size=2, stride=2)\n        self.pool2 = MaxPool2d(kernel_size=2, stride=2)\n        self.pool3 = MaxPool2d(kernel_size=(2,1), stride=(2,1))\n        self.pool4 = MaxPool2d(kernel_size=(2,1), stride=(2,1))\n        self.pool5 = MaxPool2d(kernel_size=(2,1), stride=(2,1))\n\n        self.bn1 = BatchNorm2d(64)\n        self.bn2 = BatchNorm2d(128)\n        self.bn3 = BatchNorm2d(256)\n        self.bn4 = BatchNorm2d(512)\n\n        self.rnn1 = BidirectionalLSTM(2*nHidden, nHidden, num_classes, num_layers=3)\n        self.rnn2 = BidirectionalLSTM(2*nHidden, nHidden, num_classes, num_layers=3)\n\n\n    def forward(self, src):\n        \n        x = self.act(self.bn1(self.conv0(src)))\n        x = self.act(self.bn1(self.conv1(x)))\n        x = self.pool1(x)\n        x = self.act(self.bn2(self.conv2(x)))\n        x = self.act(self.bn2(self.conv3(x)))\n        x = self.pool2(x)\n        x = self.act(self.bn3(self.conv4(x)))\n        x = self.act(self.bn3(self.conv5(x)))\n        x = self.act(self.bn3(self.conv6(x)))\n        x = self.pool3(x)\n        x = self.act(self.bn4(self.conv7(x)))\n        x = self.act(self.bn4(self.conv8(x)))\n        x = self.act(self.bn4(self.conv9(x)))\n        x = self.pool4(x)\n        x = self.act(self.bn4(self.conv10(x)))\n        x = self.act(self.bn4(self.conv11(x)))\n        x = self.act(self.bn4(self.conv12(x)))\n        x = self.pool5(x)\n        b, c, h, w = x.size()\n        assert h == 1, \"the height of conv must be 1\"\n        x = x.squeeze(2) # [b, c, h*w]\n        x = TemporalDropout(x, 0.2)\n        x = x.permute(2, 0, 1)  # [h*w, b, c]\n        output1 = self.rnn1(x)\n        output2 = self.rnn2(x)\n        output = torch.cat([output1, output2], 0)\n        output = torch.nn.functional.log_softmax(output, 2)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-07T07:24:52.386815Z","iopub.execute_input":"2022-03-07T07:24:52.387245Z","iopub.status.idle":"2022-03-07T07:24:52.41197Z","shell.execute_reply.started":"2022-03-07T07:24:52.387208Z","shell.execute_reply":"2022-03-07T07:24:52.411181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DATASET","metadata":{}},{"cell_type":"code","source":"# class for mapping symbols into indicies and vice versa\nclass LabelCoder(object):\n    def __init__(self, alphabet, ignore_case=False):\n        self.alphabet = alphabet\n        self.char2idx = {}\n        for i, char in enumerate(alphabet):\n            self.char2idx[char] = i + 1\n        self.char2idx[''] = 0\n\n    def encode(self, text: str):\n        length = []\n        result = []\n        for item in text:\n            length.append(len(item))\n            for char in item:\n                if char in self.char2idx:\n                    index = self.char2idx[char]\n                else:\n                    index = 0\n                result.append(index)\n\n        text = result\n        return (torch.IntTensor(text), torch.IntTensor(length))\n\n    def decode(self, t, length, raw=False):\n        if length.numel() == 1:\n            length = length[0]\n            assert t.numel() == length, \"text with length: {} does not match declared length: {}\".format(t.numel(),\n                                                                                                         length)\n            if raw:\n                return ''.join([self.alphabet[i - 1] for i in t])\n            else:\n                char_list = []\n                for i in range(length):\n                    if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):\n                        char_list.append(self.alphabet[t[i] - 1])\n                return ''.join(char_list)\n        else:\n            # batch mode\n            assert t.numel() == length.sum(), \"texts with length: {} does not match declared length: {}\".format(\n                t.numel(), length.sum())\n            texts = []\n            index = 0\n            for i in range(length.numel()):\n                l = length[i]\n                texts.append(\n                    self.decode(t[index:index + l], torch.IntTensor([l]), raw=raw))\n                index += l\n            return texts\n\n        \nclass OCRdataset(Dataset):\n    def __init__(self, path_to_imgdir: str, path_to_labels: str, transform_list = None):\n        super(OCRdataset, self).__init__()\n        self.imgdir = path_to_imgdir\n        df = pd.read_csv(path_to_labels, sep = '\\t', names = ['image_name', 'label'])\n        self.image2label = [(self.imgdir + image, label) for image, label in zip(df['image_name'], df['label'])]\n        if transform_list == None:\n            transform_list =  [transforms.Grayscale(1),\n                              transforms.Resize((64, 256)),\n                              transforms.ToTensor(), \n                              transforms.Normalize((0.5,), (0.5,))]\n        self.transform = transforms.Compose(transform_list)\n        self.collate_fn = Collator()\n\n    def __len__(self):\n        return len(self.image2label)\n\n    def __getitem__(self, index):\n        assert index <= len(self), 'index range error'\n        image_path, label = self.image2label[index]\n        img = Image.open(image_path)\n        if self.transform is not None:\n            img = self.transform(img)\n        item = {'idx' : index, 'img': img, 'label': label}\n        return item\n\n\nclass Collator(object):\n    \n    def __call__(self, batch):\n        width = [item['img'].shape[2] for item in batch]\n        indexes = [item['idx'] for item in batch]\n        imgs = torch.ones([len(batch), batch[0]['img'].shape[0], batch[0]['img'].shape[1], \n                           max(width)], dtype=torch.float32)\n        for idx, item in enumerate(batch):\n            try:\n                imgs[idx, :, :, 0:item['img'].shape[2]] = item['img']\n            except:\n                print(imgs.shape)\n        item = {'img': imgs, 'idx':indexes}\n        if 'label' in batch[0].keys():\n            labels = [item['label'] for item in batch]\n            item['label'] = labels\n        return item","metadata":{"execution":{"iopub.status.busy":"2022-03-07T07:24:52.414023Z","iopub.execute_input":"2022-03-07T07:24:52.414232Z","iopub.status.idle":"2022-03-07T07:24:52.438106Z","shell.execute_reply.started":"2022-03-07T07:24:52.414208Z","shell.execute_reply":"2022-03-07T07:24:52.437439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## AUGMENTATIONS\n\nThe following augmentations are used:\n\n1. *Vignetting*: a reduction of an image's brightness or saturation toward the periphery compared to the image center.\n\n2. Lens Distortion","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"import Augmentor\n\n\nclass Vignetting(object):\n    def __init__(self,\n                 p = 0.1,\n                 ratio_min_dist=0.2,\n                 range_vignette=(0.2, 0.8),\n                 random_sign=False):\n        self.ratio_min_dist = ratio_min_dist\n        self.range_vignette = np.array(range_vignette)\n        self.random_sign = random_sign\n        self.p = p\n\n    def __call__(self, X, Y=None):\n\n        if np.random.binomial(1, self.p) == 0:\n            return X \n        h, w = X.shape[1:]\n        min_dist = np.array([h, w]) / 2 * np.random.random() * self.ratio_min_dist\n        # create matrix of distance from the center on the two axis\n        x, y = np.meshgrid(np.linspace(-w / 2, w / 2, w), np.linspace(-h / 2, h / 2, h))\n        x, y = np.abs(x), np.abs(y)\n        # create the vignette mask on the two axis\n        x = (x - min_dist[0]) / (np.max(x) - min_dist[0])\n        x = np.clip(x, 0, 1)\n        y = (y - min_dist[1]) / (np.max(y) - min_dist[1])\n        y = np.clip(y, 0, 1)\n        # then get a random intensity of the vignette\n        vignette = (x + y) / 2 * np.random.uniform(*self.range_vignette)\n        vignette = np.tile(vignette[None, ...], [1, 1, 1])\n\n        sign = 2 * (np.random.random() < 0.5) * (self.random_sign) - 1\n        Z = X * (1 + sign * vignette)\n        return Z\n\n\nclass LensDistortion(object):\n    def __init__(self, p = 0.1 ,d_coef=(0.15, 0.05, 0.05, 0.05, 0.05)):\n        self.d_coef = np.array(d_coef)\n        self.p = p\n\n    def __call__(self, X):\n        if np.random.binomial(1, self.p) == 0:\n            return X \n\n        # get the height and the width of the image\n        h, w = X.shape[:2]\n\n        # compute its diagonal\n        f = (h ** 2 + w ** 2) ** 0.5\n\n        # set the image projective to carrtesian dimension\n        K = np.array([[f, 0, w / 2],\n                      [0, f, h / 2],\n                      [0, 0, 1]])\n\n        d_coef = self.d_coef * np.random.random(5)  # value\n        d_coef = d_coef * (2 * (np.random.random(5) < 0.5) - 1)  # sign\n        # Generate new camera matrix from parameters\n        M, _ = cv2.getOptimalNewCameraMatrix(K, d_coef, (w, h), 0)\n\n        # Generate look-up tables for remapping the camera image\n        remap = cv2.initUndistortRectifyMap(K, d_coef, None, M, (w, h), 5)\n\n        # Remap the original image to a new image\n        Z = cv2.remap(np.float32(X.numpy()), *remap, cv2.INTER_LINEAR)\n        return torch.from_numpy(Z)\n    \n\n\nif APPLY_AUGS:\n    transform_list = [\n            transforms.Grayscale(1),\n            transforms.Resize((64, 256)),\n            #transforms.RandomRotation(degrees=(-9, 9), fill=255),\n            #transforms.transforms.GaussianBlur(3, sigma=(0.1, 1.5)), \n            transforms.ToTensor(),\n            #Vignetting(p = 0.5),\n            #LensDistortion(p = 0.3),\n            transforms.Normalize((0.5,), (0.5,))\n        ]\nelse:\n    transform_list = None","metadata":{"execution":{"iopub.status.busy":"2022-03-07T07:24:52.439393Z","iopub.execute_input":"2022-03-07T07:24:52.439859Z","iopub.status.idle":"2022-03-07T07:24:52.463032Z","shell.execute_reply.started":"2022-03-07T07:24:52.439769Z","shell.execute_reply":"2022-03-07T07:24:52.462253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = OCRdataset(PATH_TO_TRAIN_IMGDIR, PATH_TO_TRAIN_LABELS, transform_list = transform_list)\ncollator = Collator()\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size = 8, collate_fn = collator, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T07:24:52.464389Z","iopub.execute_input":"2022-03-07T07:24:52.464774Z","iopub.status.idle":"2022-03-07T07:24:52.557444Z","shell.execute_reply.started":"2022-03-07T07:24:52.46474Z","shell.execute_reply":"2022-03-07T07:24:52.556676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Explore some examples","metadata":{}},{"cell_type":"code","source":"examples = []\nidx = 0\n\nfor batch in train_loader:\n    img, true_label = batch['img'], batch['label']\n    examples.append([img, true_label])\n    idx += 1\n    if idx == BATCH_SIZE:\n        break\nfig = plt.figure(figsize=(10, 10))\nrows = int(BATCH_SIZE / 4) + 2\ncolumns = int(BATCH_SIZE / 8) + 2\nfor j, exp in enumerate(examples):\n    fig.add_subplot(rows, columns, j + 1)\n    plt.imshow(exp[0][0].permute(2, 1, 0).permute(1, 0, 2))\n    plt.title(exp[1][0])","metadata":{"execution":{"iopub.status.busy":"2022-03-07T07:24:52.558875Z","iopub.execute_input":"2022-03-07T07:24:52.559176Z","iopub.status.idle":"2022-03-07T07:24:53.065551Z","shell.execute_reply.started":"2022-03-07T07:24:52.559141Z","shell.execute_reply":"2022-03-07T07:24:53.062509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAIN","metadata":{}},{"cell_type":"code","source":"import math\nimport torch\n\nimport math\nimport torch\n\nclass CustomCTCLoss(torch.nn.Module):\n    # T x B x H => Softmax on dimension 2\n    def __init__(self, dim=2):\n        super().__init__()\n        self.dim = dim\n        self.ctc_loss = torch.nn.CTCLoss(reduction='mean', zero_infinity=True)\n\n    def forward(self, logits, labels,\n            prediction_sizes, target_sizes):\n        EPS = 1e-7\n        loss = self.ctc_loss(logits, labels, prediction_sizes, target_sizes)\n        loss = self.sanitize(loss)\n        return self.debug(loss, logits, labels, prediction_sizes, target_sizes)\n    \n    def sanitize(self, loss):\n        EPS = 1e-7\n        if abs(loss.item()) > 99999:\n            return torch.zeros_like(loss, requires_grad = True)\n        if math.isnan(loss.item()):\n            return torch.zeros_like(loss, requires_grad = True)\n        return loss\n\n    def debug(self, loss, logits, labels,\n            prediction_sizes, target_sizes):\n        if math.isnan(loss.item()):\n            print(\"Loss:\", loss)\n            print(\"logits:\", logits)\n            print(\"labels:\", labels)\n            print(\"prediction_sizes:\", prediction_sizes)\n            print(\"target_sizes:\", target_sizes)\n            raise Exception(\"NaN loss obtained.\")\n        return loss\n\n    \ndef print_epoch_data(epoch, mean_loss, char_error, word_error, time_elapsed, zero_out_losses):\n    if epoch == 0:\n        print('epoch | mean loss | mean cer | mean wer | time elapsed | warnings')\n    epoch_str = str(epoch)\n    zero_out_losses_str = str(zero_out_losses)\n    if len(epoch_str) < 2:\n        epoch_str = '0' + epoch_str\n    if len(zero_out_losses_str) < 2:\n        zero_out_losses_str = '0' + zero_out_losses_str\n    report_line = epoch_str + ' '*7 + \"%.3f\" % mean_loss + ' '*7 + \"%.3f\" % char_error + ' '*7 + \\\n             \"%.3f\" % word_error + ' '*7 +  \"%.1f\" % float(time_elapsed)\n    if zero_out_losses != 0:\n        report_line += f'       {zero_out_losses} batch losses skipped due to nan value'\n    print(report_line)\n    \n    \ndef fit(model, optimizer, loss_fn, loader, epochs = 64):\n    report = []\n    coder = LabelCoder(ALPHABET)\n    for epoch in range(epochs):\n        zero_out_losses = 0\n        start_time = time.time()\n        model.train()\n        outputs = []\n        for batch_nb, batch in enumerate(loader):\n            optimizer.zero_grad()\n            input_, targets = batch['img'], batch['label']\n            targets, lengths = coder.encode(targets)\n            logits = model(input_.to(DEVICE))\n            logits = logits.contiguous().cpu()\n            T, B, H = logits.size()\n            pred_sizes = torch.LongTensor([T for i in range(B)])\n            targets = targets.view(-1).contiguous()\n            loss = loss_fn(logits, targets, pred_sizes, lengths)\n            if (torch.zeros(loss.size()) == loss).all():\n                zero_out_losses += 1\n                continue\n            probs, preds = logits.max(2)\n            preds = preds.transpose(1, 0).contiguous().view(-1)\n            sim_preds = coder.decode(preds.data, pred_sizes.data, raw=False)\n\n            char_error = sum([lev(batch['label'][i], sim_preds[i])/max(len(batch['label'][i]), len(sim_preds[i])) for i in range(len(batch['label']))])/len(batch['label'])\n            word_error = 1 - sum([batch['label'][i] == sim_preds[i] for i in range(len(batch['label']))])/len(batch['label'])\n\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 0.05)\n            optimizer.step()\n            output = {'loss': abs(loss.item()),'cer': char_error,'wer': word_error}\n            outputs.append(output)\n        \n        if len(outputs) == 0:\n            print('ERROR: bad loss, try to decrease learning rate and batch size')\n            return None\n        end_time = time.time()\n        mean_loss = sum([outputs[i]['loss'] for i in range(len(outputs))])/len(outputs)\n        char_error = sum([outputs[i]['cer'] for i in range(len(outputs))])/len(outputs)\n        word_error = sum([outputs[i]['wer'] for i in range(len(outputs))])/len(outputs)\n        report.append({'mean_loss' : mean_loss, 'mean_cer' : char_error, 'mean_wer' : word_error})\n        print_epoch_data(epoch, mean_loss, char_error, word_error, end_time - start_time, zero_out_losses)\n        if epoch%4 == 0:\n            torch.save(model.state_dict(), PATH_TO_CHECKPOINT + 'checkpoint_epoch_' + str(epoch) + '.pt')\n    return report ","metadata":{"execution":{"iopub.status.busy":"2022-03-07T07:24:53.066914Z","iopub.execute_input":"2022-03-07T07:24:53.067628Z","iopub.status.idle":"2022-03-07T07:24:53.093383Z","shell.execute_reply.started":"2022-03-07T07:24:53.067588Z","shell.execute_reply":"2022-03-07T07:24:53.092681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Choose an architecture","metadata":{}},{"cell_type":"code","source":"model = Model3(256, len(ALPHABET))\nmodel.to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.00002)\nloss_fn = CustomCTCLoss()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T07:24:53.094769Z","iopub.execute_input":"2022-03-07T07:24:53.095224Z","iopub.status.idle":"2022-03-07T07:24:58.58449Z","shell.execute_reply.started":"2022-03-07T07:24:53.095178Z","shell.execute_reply":"2022-03-07T07:24:58.58371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We advise to use small batch size initially but increase it by 4 each 12 epochs","metadata":{}},{"cell_type":"code","source":"report = fit(model, optimizer, loss_fn, train_loader, epochs = 84)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T07:24:58.588746Z","iopub.execute_input":"2022-03-07T07:24:58.58896Z","iopub.status.idle":"2022-03-07T14:31:46.695336Z","shell.execute_reply.started":"2022-03-07T07:24:58.588934Z","shell.execute_reply":"2022-03-07T14:31:46.694332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TEST","metadata":{}},{"cell_type":"code","source":"def evaluate(model, loader):\n    coder = LabelCoder(ALPHABET)\n    labels, predictions = [], []\n    for iteration, batch in enumerate(tqdm(loader)):\n        input_, targets = batch['img'].to(DEVICE), batch['label']\n        labels.extend(targets)\n        targets, _ = coder.encode(targets)\n        logits = model(input_)\n        logits = logits.contiguous().cpu()\n        T, B, H = logits.size()\n        pred_sizes = torch.LongTensor([T for i in range(B)])\n        probs, pos = logits.max(2)\n        pos = pos.transpose(1, 0).contiguous().view(-1)\n        sim_preds = coder.decode(pos.data, pred_sizes.data, raw=False)\n        predictions.extend(sim_preds)\n    char_error = sum([lev(labels[i], predictions[i])/max(len(labels[i]), len(predictions[i])) for i in range(len(labels))])/len(labels)\n    word_error = 1 - sum([labels[i] == predictions[i] for i in range(len(labels))])/len(labels)\n    return {'char_error' : char_error, 'word_error' : word_error}","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:33:07.796641Z","iopub.execute_input":"2022-03-07T14:33:07.7969Z","iopub.status.idle":"2022-03-07T14:33:07.807479Z","shell.execute_reply.started":"2022-03-07T14:33:07.796874Z","shell.execute_reply":"2022-03-07T14:33:07.806733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = OCRdataset(PATH_TO_TEST_IMGDIR, PATH_TO_TEST_LABELS)\ncollator = Collator()\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 2, collate_fn = collator)\nevaluate(model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:33:12.486557Z","iopub.execute_input":"2022-03-07T14:33:12.487093Z","iopub.status.idle":"2022-03-07T14:33:33.933346Z","shell.execute_reply.started":"2022-03-07T14:33:12.487058Z","shell.execute_reply":"2022-03-07T14:33:33.930559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" def predict(model, img):\n    logits = model(img.to(DEVICE))\n    logits = logits.contiguous().cpu()\n    T, B, H = logits.size()\n    pred_sizes = torch.LongTensor([T for i in range(B)])\n    probs, pos = logits.max(2)\n    pos = pos.transpose(1, 0).contiguous().view(-1)\n    sim_preds = coder.decode(pos.data, pred_sizes.data, raw=False)\n    return sim_preds","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:34:54.179028Z","iopub.execute_input":"2022-03-07T14:34:54.179719Z","iopub.status.idle":"2022-03-07T14:34:54.185595Z","shell.execute_reply.started":"2022-03-07T14:34:54.179683Z","shell.execute_reply":"2022-03-07T14:34:54.184906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"examples = []\nidx = 0\ncoder = LabelCoder(ALPHABET)\nfor batch in test_loader:\n    img, true_label = batch['img'], batch['label']\n    pred_label = predict(model, img)\n    examples.append([img, true_label, pred_label])\n    idx += 1\n    if idx == 9:\n        break\nfig = plt.figure(figsize=(10, 10))\nrows = int(9 / 4) + 2\ncolumns = int(9 / 8) + 2\nfor j, exp in enumerate(examples):\n    fig.add_subplot(rows, columns, j + 1)\n    plt.imshow(exp[0][0].permute(2, 1, 0).permute(1, 0, 2))\n    plt.title('true:' + exp[1][0] + '\\npred:' + exp[2][0], loc = 'left')","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:35:53.367492Z","iopub.execute_input":"2022-03-07T14:35:53.367766Z","iopub.status.idle":"2022-03-07T14:35:54.550826Z","shell.execute_reply.started":"2022-03-07T14:35:53.367737Z","shell.execute_reply":"2022-03-07T14:35:54.54993Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
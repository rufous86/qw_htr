# StackMix and Blot Augmentations for Handwritten Text Recognition
paper [arxiv](https://arxiv.org/abs/2108.11667)  [paperswithcode](https://paperswithcode.com/paper/stackmix-and-blot-augmentations-for)

Datasets Used [IAM](https://paperswithcode.com/dataset/iam) [HKR](https://paperswithcode.com/dataset/hkr) [HKR-Dataset](https://github.com/abdoelsayed2016/HKR_Dataset) [Digital-Peter](https://paperswithcode.com/dataset/digital-peter) [cyrillic-handwriting-dataset-kaggle](https://www.kaggle.com/datasets/constantinwerner/cyrillic-handwriting-dataset) [HKR-huggingface](https://huggingface.co/datasets/nastyboget/stackmix_hkr)

examples [StackMix-OCR-github](https://github.com/ai-forever/StackMix-OCR)

В тексте рассматриваются различные методы и подходы к распознаванию рукописного текста (HTR) с использованием моделей глубокого обучения. В начале излагаются первые работы по проблемам распознавания рукописного текста, в которых использовалась комбинация скрытых марковских моделей и рекуррентных нейронных сетей (RNN) или алгоритмы на основе условных случайных полей. Однако было обнаружено, что эти подходы имеют свои недостатки, в частности, невозможность оптимизации сквозной функции потерь.

В 2006 году был представлен новый подход, известный как коннекционистская темпоральная классификация (CTC). Этот подход интерпретирует выходные данные сети как распределение вероятностей по всем возможным последовательностям меток, обусловленным заданной входной последовательностью. Это позволяет получить целевую функцию, которая непосредственно максимизирует вероятности правильных меток. Поскольку объективная функция дифференцируема, сеть может быть обучена стандартным методом обратного распространения во времени. В рамках этого подхода были введены потери CTC, которые нашли широкое признание среди исследователей и стали стандартом де-факто для распознавания рукописных работ.

Также были упомянуты MDLSTM-сети, использующие 2D-RNN. Эти сети могут работать с обеими осями входного изображения и состоят из нескольких слоев CNN и MDLSTM. Однако эти модели имеют ряд недостатков, таких как высокая вычислительная стоимость и нестабильность. Для решения этих проблем были предложены различные методы, такие как метод "упаковки примеров" из работы [10] и исключение рекуррентных слоев в CNN-LSTM-CTC для уменьшения количества параметров ieeexplore.ieee.org.

В качестве альтернативы подходу RCNN-CTC были также предложены модели Seq2seq. Эти модели используют кодер для извлечения признаков из входного сигнала и декодер с механизмом внимания для последовательной выдачи выходного сигнала. Обычные приемы могут существенно улучшить качество HTR-моделей arxiv.org.

Далее в тексте рассматривается новый метод дополнения данных, имитирующий зачеркнутый текст, известный как Handwritten Blots. Этот метод был разработан в ходе анализа набора данных Digital Peter и предполагает использование аугментации Cutout [12] для создания эффекта, похожего на перечеркнутые символы. Реализация этого алгоритма предполагает использование алгоритма построения кривой Безье для сглаживания перехода кривой между точками arxiv.org.

Наконец, в тексте описывается предложенная модель, состоящая из трех частей: модифицированной архитектуры нейронной сети Resnet, нового метода аугментации, имитирующего зачеркнутый текст, и метода значительного увеличения объема обучающих данных за счет генерации нового текста в стиле текущего набора данных. Результаты, полученные при использовании этой архитектуры без дополнительных модификаций, приведены в табл. 4, а влияние дополнения Blot на метрики качества также обсуждается arxiv.org.

# Connectionist Temporal Classification (CTC)
Connectionist Temporal Classification (CTC) - это метод, используемый для обучения рекуррентных нейронных сетей (RNN) для обработки последовательностей данных. Этот метод особенно полезен для задач, где данные представляют собой последовательности, такие как распознавание речи или рукописного текста [ru.wikipedia.org](https://ru.wikipedia.org/wiki/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C).

CTC представляет новую функцию потерь, которая позволяет сетям RNN напрямую использовать обучение памяти несегментированных последовательностей. Это означает, что CTC может работать с последовательностями данных, которые не были предварительно разделены на отдельные сегменты или "блоки". Вместо этого, CTC позволяет сетям RNN учиться на основе всего входного последовательного потока данных [russianblogs.com](https://www.russianblogs.com/article/3121270981/).

В контексте CTC, "соединитель" относится к способности сети учиться на основе последовательности входных данных, а "временной" относится к способности сети учиться на основе последовательности во времени. "Классификация" означает, что CTC используется для задач классификации, где выходной результат - это определенный класс или категория.

Важно отметить, что CTC не предназначен для прогнозирования последовательности, поскольку он может только прогнозировать классификацию некоторых независимых меток. Однако, CTC обеспечивает мощный общий механизм для построения временных рядов и очень устойчив к временным и пространственным шумам.

Важным аспектом CTC является использование "пустой" метки. Это специальная метка, которая может быть выведена на RNN. Выход RNN - это вероятность всех меток. Это позволяет CTC учиться на основе всего входного последовательного потока данных, а не только на отдельных сегментах.

В целом, CTC представляет собой мощный инструмент для обучения рекуррентных нейронных сетей на последовательных данных. Он обеспечивает гибкость и устойчивость к шумам, что делает его полезным для многих задач обработки последовательностей.

Connectionist Temporal Classification (CTC) - это функция потерь, используемая для обучения рекуррентных нейронных сетей (RNN) для маркировки неразделенных входных последовательностей данных в обучении с учителем.

В контексте распознавания речи, например, с использованием типичной функции потерь кросс-энтропии, входной сигнал должен быть разделен на слова или подслова. Однако, используя функцию потерь CTC, достаточно предоставить одну последовательность меток для входной последовательности, и сеть учится как выравнивать, так и маркировать.

CTC позволяет добиться как упорядочивания, так и распознавания. Многие рекуррентные сети используют стеки данных, присущие CTC, чтобы найти матрицу весов, в которой вероятность последовательности меток в наборе образцов при соответствующем входном потоке сводится к максимуму.

В MXNet реализована функция CTC-loss, которая включена в стандартный пакет. Также есть возможность использовать функцию CTC-loss с помощью библиотеки Baidu's warp-ctc, что требует сборки обеих библиотек из исходного кода [mxnet.apache.org](https://mxnet.apache.org/versions/1.2.1/tutorials/speech_recognition/ctc.html).

В качестве примера, MXNet предоставляет пример использования CTC-loss с сетью LSTM для выполнения предсказания распознавания текста на изображениях CAPTCHA. Этот пример демонстрирует использование обоих вариантов CTC-loss, а также инференции после обучения с использованием контрольных точек символа и параметров сети.
